{"cells":[{"cell_type":"code","execution_count":9,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/rsna-str-pulmonary-embolism-detection/test.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-57b89a22b6ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;31m####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/rsna-str-pulmonary-embolism-detection/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0mstudy_id_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'StudyInstanceUID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0mseries_id_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SeriesInstanceUID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/ubuntu/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/ubuntu/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/ubuntu/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/ubuntu/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/ubuntu/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/rsna-str-pulmonary-embolism-detection/test.csv'"]}],"metadata":{"collapsed":false,"_kg_hide-input":false},"source":["# import subprocess\n","# subprocess.run([\"tar\", \"-xvf\", \"../input/gdcminstall/gdcm.tar\"])\n","# subprocess.run([\"conda\", \"install\", \"../working/gdcm/conda-4.8.4-py37hc8dfbb8_2.tar.bz2\"])\n","# subprocess.run([\"conda\", \"install\", \"../working/gdcm/gdcm-2.8.9-py37h71b2a6d_0.tar.bz2\"])\n","# subprocess.run([\"conda\", \"install\", \"../working/gdcm/libjpeg-turbo-2.0.3-h516909a_1.tar.bz2\"])\n","\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import os\n","from tqdm import tqdm\n","import glob\n","import pickle\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import sys\n","sys.path.insert(0, '/home/ubuntu/efs/project/RSNA-STR-Pulmonary-Embolism-Detection/trainall/lung_localization/splitall/')#'../input/efficientnetpytorch/')\n","#print(sys.path)\n","from efficientnet_pytorch import EfficientNet\n","sys.path.insert(0, '/home/ubuntu/efs/project/RSNA-STR-Pulmonary-Embolism-Detection/trainall/seresnext50/')\n","from pretrainedmodels.senet import se_resnext101_32x4d, se_resnext50_32x4d\n","import pydicom\n","\n","def correct_predictions(pred_prob_list, series_len_list):\n","    eps = 0.000001\n","    pred_prob_list_corrected = np.zeros(pred_prob_list.shape, dtype=np.float32)\n","    start = 0\n","    for i in tqdm(range(len(series_len_list))):\n","        end = series_len_list[i]\n","        negative_exam_for_pe = pred_prob_list[start+0]\n","        indeterminate = pred_prob_list[start+1]\n","        chronic_pe = pred_prob_list[start+2]\n","        acute_and_chronic_pe = pred_prob_list[start+3]\n","        central_pe = pred_prob_list[start+4]\n","        leftsided_pe = pred_prob_list[start+5]\n","        rightsided_pe = pred_prob_list[start+6]\n","        rv_lv_ratio_gte_1 = pred_prob_list[start+7]\n","        rv_lv_ratio_lt_1 = pred_prob_list[start+8]\n","        image_pe = pred_prob_list[start+9:end]\n","\n","        loss_weight_list = np.zeros(pred_prob_list[start:end].shape, dtype=np.float32)\n","        loss_weight_list[0] = 0.0736196319\n","        loss_weight_list[1] = 0.09202453988\n","        loss_weight_list[2] = 0.1042944785\n","        loss_weight_list[3] = 0.1042944785\n","        loss_weight_list[4] = 0.1877300613\n","        loss_weight_list[5] = 0.06257668712\n","        loss_weight_list[6] = 0.06257668712\n","        loss_weight_list[7] = 0.2346625767\n","        loss_weight_list[8] = 0.0782208589\n","        loss_weight_list[9:] = 0.07361963*0.005\n","        \n","        if (np.amax(image_pe)<=0.5) and (int(negative_exam_for_pe>0.5)+int(indeterminate>0.5)==1) and (int(chronic_pe>0.5)+int(acute_and_chronic_pe>0.5)==0) and (int(central_pe>0.5)+int(leftsided_pe>0.5)+int(rightsided_pe>0.5)==0) and (int(rv_lv_ratio_gte_1>0.5)+int(rv_lv_ratio_lt_1>0.5)==0):\n","            pred_prob_list_corrected[start:end] = pred_prob_list[start:end]\n","        elif (np.amax(image_pe)>0.5) and (int(negative_exam_for_pe>0.5)+int(indeterminate>0.5)==0) and (int(chronic_pe>0.5)+int(acute_and_chronic_pe>0.5)<2) and (int(central_pe>0.5)+int(leftsided_pe>0.5)+int(rightsided_pe>0.5)>0) and (int(rv_lv_ratio_gte_1>0.5)+int(rv_lv_ratio_lt_1>0.5)==1):\n","            pred_prob_list_corrected[start:end] = pred_prob_list[start:end]\n","        else:\n","            to_neg = pred_prob_list[start:end].copy()\n","            for n in range(len(image_pe)):\n","                if image_pe[n]>0.5:\n","                    to_neg[9+n] = 0.5\n","            if negative_exam_for_pe>0.5 and indeterminate>0.5:\n","                if negative_exam_for_pe>indeterminate:\n","                    to_neg[1] = 0.5\n","                else:\n","                    to_neg[0] = 0.5\n","            elif negative_exam_for_pe<=0.5 and indeterminate<=0.5:\n","                if negative_exam_for_pe>indeterminate:\n","                    to_neg[0] = 0.5+eps\n","                else:\n","                    to_neg[1] = 0.5+eps\n","            if chronic_pe>0.5:\n","                to_neg[2] = 0.5\n","            if acute_and_chronic_pe>0.5:\n","                to_neg[3] = 0.5\n","            if central_pe>0.5:\n","                to_neg[4] = 0.5\n","            if leftsided_pe>0.5:\n","                to_neg[5] = 0.5\n","            if rightsided_pe>0.5:\n","                to_neg[6] = 0.5\n","            if rv_lv_ratio_gte_1>0.5:\n","                to_neg[7] = 0.5\n","            if rv_lv_ratio_lt_1>0.5:\n","                to_neg[8] = 0.5\n","\n","            to_pos = pred_prob_list[start:end].copy()\n","            if np.amax(image_pe)<=0.5:\n","                max_idx = np.argmax(image_pe)\n","                to_pos[9+max_idx] = 0.5+eps\n","            if negative_exam_for_pe>0.5:\n","                to_pos[0] = 0.5\n","            if indeterminate>0.5:\n","                to_pos[1] = 0.5\n","            if chronic_pe>0.5 and acute_and_chronic_pe>0.5:\n","                if chronic_pe>acute_and_chronic_pe:\n","                    to_pos[3] = 0.5\n","                else:\n","                    to_pos[2] = 0.5\n","            if central_pe<=0.5 and leftsided_pe<=0.5 and rightsided_pe<=0.5:\n","                if central_pe>leftsided_pe and central_pe>rightsided_pe:\n","                    to_pos[4] = 0.5+eps\n","                if leftsided_pe>central_pe and leftsided_pe>rightsided_pe:\n","                    to_pos[5] = 0.5+eps\n","                if rightsided_pe>central_pe and rightsided_pe>leftsided_pe:\n","                    to_pos[6] = 0.5+eps\n","            if rv_lv_ratio_gte_1>0.5 and rv_lv_ratio_lt_1>0.5:\n","                if rv_lv_ratio_gte_1>rv_lv_ratio_lt_1:\n","                    to_pos[8] = 0.5\n","                else:\n","                    to_pos[7] = 0.5\n","            elif rv_lv_ratio_gte_1<=0.5 and rv_lv_ratio_lt_1<=0.5:\n","                if rv_lv_ratio_gte_1>rv_lv_ratio_lt_1:\n","                    to_pos[7] = 0.5+eps\n","                else:\n","                    to_pos[8] = 0.5+eps\n","\n","            loss_weight_list1 = torch.tensor(loss_weight_list, dtype=torch.float32)\n","            pred_prob_list1 = torch.tensor(pred_prob_list[start:end], dtype=torch.float32)\n","            pred_prob_list_neg = torch.tensor(to_neg, dtype=torch.float32)\n","            pred_prob_list_pos = torch.tensor(to_pos, dtype=torch.float32)\n","            #print(loss_weight_list1.shape, pred_prob_list1.shape, pred_prob_list_neg.shape, pred_prob_list_pos.shape)\n","            to_neg_loss = ((torch.nn.BCELoss(reduction='none')(pred_prob_list1, pred_prob_list_neg)*loss_weight_list1).sum() / loss_weight_list1.sum()).numpy()\n","            to_pos_loss = ((torch.nn.BCELoss(reduction='none')(pred_prob_list1, pred_prob_list_pos)*loss_weight_list1).sum() / loss_weight_list1.sum()).numpy()\n","\n","            if to_neg_loss>to_pos_loss:\n","                pred_prob_list_corrected[start:end] = to_pos\n","            else:\n","                pred_prob_list_corrected[start:end] = to_neg\n","\n","        start = series_len_list[i]\n","    return pred_prob_list_corrected\n","\n","def check_consistency(sub, test):\n","    \n","    '''\n","    Checks label consistency and returns the errors\n","    \n","    Args:\n","    sub   = submission dataframe (pandas)\n","    test  = test.csv dataframe (pandas)\n","    '''\n","    \n","    # EXAM LEVEL\n","    for i in test['StudyInstanceUID'].unique():\n","        df_tmp = sub.loc[sub.id.str.contains(i, regex = False)].reset_index(drop = True)\n","        df_tmp['StudyInstanceUID'] = df_tmp['id'].str.split('_').str[0]\n","        df_tmp['label_type']       = df_tmp['id'].str.split('_').str[1:].apply(lambda x: '_'.join(x))\n","        del df_tmp['id']\n","        if i == test['StudyInstanceUID'].unique()[0]:\n","            df = df_tmp.copy()\n","        else:\n","            df = pd.concat([df, df_tmp], axis = 0)\n","    df_exam = df.pivot(index = 'StudyInstanceUID', columns = 'label_type', values = 'label')\n","    \n","    # IMAGE LEVEL\n","    df_image = sub.loc[sub.id.isin(test.SOPInstanceUID)].reset_index(drop = True)\n","    df_image = df_image.merge(test, how = 'left', left_on = 'id', right_on = 'SOPInstanceUID')\n","    df_image.rename(columns = {\"label\": \"pe_present_on_image\"}, inplace = True)\n","    del df_image['id']\n","    \n","    # MERGER\n","    df = df_exam.merge(df_image, how = 'left', on = 'StudyInstanceUID')\n","    ids    = ['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID']\n","    labels = [c for c in df.columns if c not in ids]\n","    df = df[ids + labels]\n","    \n","    # SPLIT NEGATIVE AND POSITIVE EXAMS\n","    df['positive_images_in_exam'] = df['StudyInstanceUID'].map(df.groupby(['StudyInstanceUID']).pe_present_on_image.max())\n","    df_pos = df.loc[df.positive_images_in_exam >  0.5]\n","    df_neg = df.loc[df.positive_images_in_exam <= 0.5]\n","    \n","    # CHECKING CONSISTENCY OF POSITIVE EXAM LABELS\n","    rule1a = df_pos.loc[((df_pos.rv_lv_ratio_lt_1  >  0.5)  & \n","                         (df_pos.rv_lv_ratio_gte_1 >  0.5)) | \n","                        ((df_pos.rv_lv_ratio_lt_1  <= 0.5)  & \n","                         (df_pos.rv_lv_ratio_gte_1 <= 0.5))].reset_index(drop = True)\n","    rule1a['broken_rule'] = '1a'\n","    rule1b = df_pos.loc[(df_pos.central_pe    <= 0.5) & \n","                        (df_pos.rightsided_pe <= 0.5) & \n","                        (df_pos.leftsided_pe  <= 0.5)].reset_index(drop = True)\n","    rule1b['broken_rule'] = '1b'\n","    rule1c = df_pos.loc[(df_pos.acute_and_chronic_pe > 0.5) & \n","                        (df_pos.chronic_pe           > 0.5)].reset_index(drop = True)\n","    rule1c['broken_rule'] = '1c'\n","    rule1d = df_pos.loc[(df_pos.indeterminate        > 0.5) | \n","                        (df_pos.negative_exam_for_pe > 0.5)].reset_index(drop = True)\n","    rule1d['broken_rule'] = '1d'\n","\n","    # CHECKING CONSISTENCY OF NEGATIVE EXAM LABELS\n","    rule2a = df_neg.loc[((df_neg.indeterminate        >  0.5)  & \n","                         (df_neg.negative_exam_for_pe >  0.5)) | \n","                        ((df_neg.indeterminate        <= 0.5)  & \n","                         (df_neg.negative_exam_for_pe <= 0.5))].reset_index(drop = True)\n","    rule2a['broken_rule'] = '2a'\n","    rule2b = df_neg.loc[(df_neg.rv_lv_ratio_lt_1     > 0.5) | \n","                        (df_neg.rv_lv_ratio_gte_1    > 0.5) |\n","                        (df_neg.central_pe           > 0.5) | \n","                        (df_neg.rightsided_pe        > 0.5) | \n","                        (df_neg.leftsided_pe         > 0.5) |\n","                        (df_neg.acute_and_chronic_pe > 0.5) | \n","                        (df_neg.chronic_pe           > 0.5)].reset_index(drop = True)\n","    rule2b['broken_rule'] = '2b'\n","    \n","    # MERGING INCONSISTENT PREDICTIONS\n","    errors = pd.concat([rule1a, rule1b, rule1c, rule1d, rule2a, rule2b], axis = 0)\n","    \n","    # OUTPUT\n","    print('Found', len(errors), 'inconsistent predictions')\n","    return errors\n","\n","####################################\n","df = pd.read_csv('../input/test.csv')\n","study_id_list = df['StudyInstanceUID'].values\n","series_id_list = df['SeriesInstanceUID'].values\n","image_id_list = df['SOPInstanceUID'].values\n","series_list = []\n","series_dict = {}\n","image_dict = {}\n","for i in range(len(series_id_list)):\n","    series_id = study_id_list[i]+'_'+series_id_list[i]\n","    image_id = image_id_list[i]\n","    series_dict[series_id] = {'sorted_image_list': []}\n","    series_list.append(series_id)\n","    image_dict[image_id] = {'series_id': series_id, 'image_minus1': '', 'image_plus1': ''}\n","series_list = sorted(list(set(series_list)))\n","print(len(series_list), len(series_dict), len(image_dict))\n","######################################\n","\n","\n","#######################################\n","class BboxDataset(Dataset):\n","    def __init__(self, series_list):\n","        self.series_list = series_list\n","    def __len__(self):\n","        return len(self.series_list)\n","    def __getitem__(self,index):\n","        return index\n","\n","class BboxCollator(object):\n","    def __init__(self, series_list):\n","        self.series_list = series_list\n","    def _window(self, x, WL=50, WW=350):\n","        upper, lower = WL+WW//2, WL-WW//2\n","        x = np.clip(x, lower, upper)\n","        x = x - np.min(x)\n","        x = x / np.max(x)\n","        return x\n","    def _load_dicom_array(self, f):\n","        dicom_files = glob.glob(os.path.join(f, '*.dcm'))\n","        dicoms = [pydicom.dcmread(d) for d in dicom_files]\n","        M = np.float32(dicoms[0].RescaleSlope)\n","        B = np.float32(dicoms[0].RescaleIntercept)\n","        z_pos = [float(d.ImagePositionPatient[-1]) for d in dicoms]\n","        sorted_idx = np.argsort(z_pos)\n","        dicom_files = np.asarray(dicom_files)[sorted_idx]\n","        dicoms = np.asarray(dicoms)[sorted_idx]\n","        selected_idx = [int(0.2*len(dicom_files)), int(0.3*len(dicom_files)), int(0.4*len(dicom_files)), int(0.5*len(dicom_files))]\n","        selected_dicom_files = dicom_files[selected_idx]\n","        selected_dicoms = dicoms[selected_idx]\n","        dicoms = np.asarray([d.pixel_array.astype(np.float32) for d in selected_dicoms])\n","        dicoms = dicoms * M\n","        dicoms = dicoms + B\n","        dicoms = self._window(dicoms, WL=100, WW=700)\n","        return dicoms, dicom_files\n","    def __call__(self, batch_idx):\n","        study_id = self.series_list[batch_idx[0]].split('_')[0]\n","        series_id = self.series_list[batch_idx[0]].split('_')[1]\n","        series_dir = '../input/rsna-str-pulmonary-embolism-detection/test/' + study_id + '/'+ series_id\n","        dicoms, dicom_files = self._load_dicom_array(series_dir)\n","        sorted_image_list = []\n","        for i in range(len(dicom_files)):\n","            name = dicom_files[i][-16:-4]\n","            sorted_image_list.append(name)\n","        x = np.zeros((4, 3, dicoms.shape[1], dicoms.shape[2]), dtype=np.float32)\n","        for i in range(4):\n","            x[i,0] = dicoms[i]\n","            x[i,1] = dicoms[i]\n","            x[i,2] = dicoms[i]\n","        return torch.from_numpy(x), sorted_image_list, self.series_list[batch_idx[0]]\n","\n","class bbox_efficientnet(nn.Module):\n","    def __init__(self ):\n","        super().__init__()\n","        self.net = EfficientNet.from_name('efficientnet-b0')\n","        in_features = self.net._fc.in_features\n","        self.last_linear = nn.Linear(in_features, 4)\n","    def forward(self, x):\n","        x = self.net.extract_features(x)\n","        x = self.net._avg_pooling(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.last_linear(x)\n","        return x\n","    \n","model_bbox = bbox_efficientnet()\n","model_bbox.load_state_dict(torch.load('../input/lungdetector/splitall/epoch34_polyak'))\n","model_bbox = model_bbox.cuda()\n","model_bbox.eval()\n","\n","bbox_dict = {}\n","\n","datagen = BboxDataset(series_list=series_list)\n","collate_fn = BboxCollator(series_list=series_list)\n","generator = DataLoader(dataset=datagen, collate_fn=collate_fn, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n","total_steps = len(generator)\n","for i, (images, sorted_image_list, series_id) in tqdm(enumerate(generator), total=total_steps):\n","    with torch.no_grad():\n","        start = i*4\n","        end = start+4\n","        if i == len(generator)-1:\n","            end = len(generator.dataset)*4\n","        images = images.cuda()\n","        logits = model_bbox(images)\n","        bbox = np.squeeze(logits.cpu().data.numpy())\n","        xmin = np.round(min([bbox[0,0], bbox[1,0], bbox[2,0], bbox[3,0]])*512)\n","        ymin = np.round(min([bbox[0,1], bbox[1,1], bbox[2,1], bbox[3,1]])*512)\n","        xmax = np.round(max([bbox[0,2], bbox[1,2], bbox[2,2], bbox[3,2]])*512)\n","        ymax = np.round(max([bbox[0,3], bbox[1,3], bbox[2,3], bbox[3,3]])*512)\n","        bbox_dict[series_id] = [int(max(0, xmin)), int(max(0, ymin)), int(min(512, xmax)), int(min(512, ymax))]\n","        for j in range(len(sorted_image_list)):\n","            name = sorted_image_list[j]\n","            if j==0:\n","                image_dict[name]['image_minus1'] = name\n","                image_dict[name]['image_plus1'] = sorted_image_list[j+1]\n","            elif j==len(sorted_image_list)-1:\n","                image_dict[name]['image_minus1'] = sorted_image_list[j-1]\n","                image_dict[name]['image_plus1'] = name\n","            else:\n","                image_dict[name]['image_minus1'] = sorted_image_list[j-1]\n","                image_dict[name]['image_plus1'] = sorted_image_list[j+1]\n","        series_dict[series_id]['sorted_image_list'] = sorted_image_list\n","\n","print(len(bbox_dict), len(series_dict), len(image_dict))\n","print(bbox_dict[series_list[0]])\n","print(series_dict[series_list[0]])\n","print(image_dict[sorted_image_list[0]])\n","####################################\n","\n","\n","####################################\n","class PEDataset(Dataset):\n","    def __init__(self, image_dict, bbox_dict, image_list, target_size):\n","        self.image_dict=image_dict\n","        self.bbox_dict=bbox_dict\n","        self.image_list=image_list\n","        self.target_size=target_size\n","    def _window(self, img, WL=50, WW=350):\n","        upper, lower = WL+WW//2, WL-WW//2\n","        X = np.clip(img.copy(), lower, upper)\n","        X = X - np.min(X)\n","        X = X / np.max(X)\n","        X = (X*255.0).astype('uint8')\n","        return X\n","    def __len__(self):\n","        return len(self.image_list)\n","    def __getitem__(self,index):\n","        study_id = self.image_dict[self.image_list[index]]['series_id'].split('_')[0]\n","        series_id = self.image_dict[self.image_list[index]]['series_id'].split('_')[1]\n","        data1 = pydicom.dcmread('../input/rsna-str-pulmonary-embolism-detection/test/'+study_id+'/'+series_id+'/'+self.image_dict[self.image_list[index]]['image_minus1']+'.dcm')\n","        data2 = pydicom.dcmread('../input/rsna-str-pulmonary-embolism-detection/test/'+study_id+'/'+series_id+'/'+self.image_list[index]+'.dcm')\n","        data3 = pydicom.dcmread('../input/rsna-str-pulmonary-embolism-detection/test/'+study_id+'/'+series_id+'/'+self.image_dict[self.image_list[index]]['image_plus1']+'.dcm')\n","        x1 = data1.pixel_array\n","        x2 = data2.pixel_array\n","        x3 = data3.pixel_array\n","        x1 = x1*data1.RescaleSlope+data1.RescaleIntercept\n","        x2 = x2*data2.RescaleSlope+data2.RescaleIntercept\n","        x3 = x3*data3.RescaleSlope+data3.RescaleIntercept\n","        x1 = np.expand_dims(self._window(x1, WL=100, WW=700), axis=2)\n","        x2 = np.expand_dims(self._window(x2, WL=100, WW=700), axis=2)\n","        x3 = np.expand_dims(self._window(x3, WL=100, WW=700), axis=2)\n","        x = np.concatenate([x1, x2, x3], axis=2)\n","        bbox = self.bbox_dict[self.image_dict[self.image_list[index]]['series_id']]\n","        x = x[bbox[1]:bbox[3],bbox[0]:bbox[2],:]\n","        x = cv2.resize(x, (self.target_size,self.target_size))\n","        x = transforms.ToTensor()(x)\n","        x = transforms.Normalize(mean=[0.456, 0.456, 0.456], std=[0.224, 0.224, 0.224])(x)\n","        return x\n","    \n","class pe_seresnext101(nn.Module):\n","    def __init__(self ):\n","        super().__init__()\n","        self.net = se_resnext101_32x4d(num_classes=1000, pretrained=None)\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        in_features = self.net.last_linear.in_features\n","        self.last_linear = nn.Linear(in_features, 1)\n","    def forward(self, x):\n","        x = self.net.features(x)\n","        x = self.avg_pool(x)\n","        feature = x.view(x.size(0), -1)\n","        x = self.last_linear(feature)\n","        return feature, x\n","    \n","class pe_seresnext50(nn.Module):\n","    def __init__(self ):\n","        super().__init__()\n","        self.net = se_resnext50_32x4d(num_classes=1000, pretrained=None)\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        in_features = self.net.last_linear.in_features\n","        self.last_linear = nn.Linear(in_features, 1)\n","    def forward(self, x):\n","        x = self.net.features(x)\n","        x = self.avg_pool(x)\n","        feature = x.view(x.size(0), -1)\n","        x = self.last_linear(feature)\n","        return feature, x\n","    \n","class lv2Dataset(Dataset):\n","    def __init__(self,\n","                 feature_array,\n","                 feature_array1,\n","                 image_to_feature,\n","                 series_dict,\n","                 image_dict,\n","                 series_list,\n","                 seq_len):\n","        self.feature_array=feature_array\n","        self.feature_array1=feature_array1\n","        self.image_to_feature=image_to_feature\n","        self.series_dict=series_dict\n","        self.image_dict=image_dict\n","        self.series_list=series_list\n","        self.seq_len=seq_len\n","    def __len__(self):\n","        return len(self.series_list)\n","    def __getitem__(self,index):\n","        image_list = self.series_dict[self.series_list[index]]['sorted_image_list'] \n","        if len(image_list)>self.seq_len:\n","            x = np.zeros((len(image_list), self.feature_array.shape[1]*3), dtype=np.float32)\n","            x1 = np.zeros((len(image_list), self.feature_array.shape[1]*3), dtype=np.float32)\n","            mask = np.ones((self.seq_len,), dtype=np.float32)\n","            for i in range(len(image_list)):      \n","                x[i,:self.feature_array.shape[1]] = self.feature_array[self.image_to_feature[image_list[i]]].copy() \n","                x1[i,:self.feature_array.shape[1]] = self.feature_array1[self.image_to_feature[image_list[i]]].copy() \n","            x = cv2.resize(x, (self.feature_array.shape[1]*3, self.seq_len), interpolation = cv2.INTER_LINEAR)\n","            x1 = cv2.resize(x1, (self.feature_array.shape[1]*3, self.seq_len), interpolation = cv2.INTER_LINEAR)\n","        else:\n","            x = np.zeros((self.seq_len, self.feature_array.shape[1]*3), dtype=np.float32)\n","            x1 = np.zeros((self.seq_len, self.feature_array.shape[1]*3), dtype=np.float32)\n","            mask = np.zeros((self.seq_len,), dtype=np.float32)\n","            for i in range(len(image_list)):      \n","                x[i,:self.feature_array.shape[1]] = self.feature_array[self.image_to_feature[image_list[i]]].copy()\n","                x1[i,:self.feature_array.shape[1]] = self.feature_array1[self.image_to_feature[image_list[i]]].copy()\n","                mask[i] = 1.\n","        x[1:,self.feature_array.shape[1]:self.feature_array.shape[1]*2] = x[1:,:self.feature_array.shape[1]] - x[:-1,:self.feature_array.shape[1]]\n","        x[:-1,self.feature_array.shape[1]*2:] = x[:-1,:self.feature_array.shape[1]] - x[1:,:self.feature_array.shape[1]]\n","        x = torch.tensor(x, dtype=torch.float32)\n","        x1[1:,self.feature_array.shape[1]:self.feature_array.shape[1]*2] = x1[1:,:self.feature_array.shape[1]] - x1[:-1,:self.feature_array.shape[1]]\n","        x1[:-1,self.feature_array.shape[1]*2:] = x1[:-1,:self.feature_array.shape[1]] - x1[1:,:self.feature_array.shape[1]]\n","        x1 = torch.tensor(x1, dtype=torch.float32)\n","        mask = torch.tensor(mask, dtype=torch.float32)\n","        return x, x1, mask, self.series_list[index]\n","\n","class SpatialDropout(nn.Dropout2d):\n","    def forward(self, x):\n","        x = x.unsqueeze(2)    # (N, T, 1, K)\n","        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n","        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n","        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n","        x = x.squeeze(2)  # (N, T, K)\n","        return x\n","    \n","# https://www.kaggle.com/bminixhofer/a-validation-framework-impact-of-the-random-seed\n","class Attention(nn.Module):\n","    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n","        super(Attention, self).__init__(**kwargs)\n","        \n","        self.supports_masking = True\n","\n","        self.bias = bias\n","        self.feature_dim = feature_dim\n","        self.step_dim = step_dim\n","        self.features_dim = 0\n","        \n","        weight = torch.zeros(feature_dim, 1)\n","        nn.init.xavier_uniform_(weight)\n","        self.weight = nn.Parameter(weight)\n","        \n","        if bias:\n","            self.b = nn.Parameter(torch.zeros(step_dim))\n","        \n","    def forward(self, x, mask=None):\n","        feature_dim = self.feature_dim\n","        step_dim = self.step_dim\n","\n","        eij = torch.mm(\n","            x.contiguous().view(-1, feature_dim), \n","            self.weight\n","        ).view(-1, step_dim)\n","        \n","        if self.bias:\n","            eij = eij + self.b\n","            \n","        eij = torch.tanh(eij)\n","        a = torch.exp(eij)\n","        \n","        if mask is not None:\n","            a = a * mask\n","\n","        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n","\n","        weighted_input = x * torch.unsqueeze(a, -1)\n","        return torch.sum(weighted_input, 1)\n","\n","class lv2Net(nn.Module):\n","    def __init__(self, input_len, lstm_size):\n","        super().__init__()\n","        self.lstm1 = nn.GRU(input_len, lstm_size, bidirectional=True, batch_first=True)\n","        self.last_linear_pe = nn.Linear(lstm_size*2, 1)\n","        self.last_linear_npe = nn.Linear(lstm_size*4, 1)\n","        self.last_linear_idt = nn.Linear(lstm_size*4, 1)\n","        self.last_linear_lpe = nn.Linear(lstm_size*4, 1)\n","        self.last_linear_rpe = nn.Linear(lstm_size*4, 1)\n","        self.last_linear_cpe = nn.Linear(lstm_size*4, 1)\n","        self.last_linear_gte = nn.Linear(lstm_size*4, 1)\n","        self.last_linear_lt = nn.Linear(lstm_size*4, 1)\n","        self.last_linear_chronic = nn.Linear(lstm_size*4, 1)\n","        self.last_linear_acute_and_chronic = nn.Linear(lstm_size*4, 1)\n","        self.attention = Attention(lstm_size*2, seq_len)\n","    def forward(self, x, mask):\n","        #x = SpatialDropout(0.5)(x)\n","        h_lstm1, _ = self.lstm1(x)\n","        logits_pe = self.last_linear_pe(h_lstm1)\n","        max_pool, _ = torch.max(h_lstm1, 1)\n","        att_pool = self.attention(h_lstm1, mask)\n","        conc = torch.cat((max_pool, att_pool), 1)\n","        logits_npe = self.last_linear_npe(conc)\n","        logits_idt = self.last_linear_idt(conc)\n","        logits_lpe = self.last_linear_lpe(conc)\n","        logits_rpe = self.last_linear_rpe(conc)\n","        logits_cpe = self.last_linear_cpe(conc)\n","        logits_gte = self.last_linear_gte(conc)\n","        logits_lt = self.last_linear_lt(conc)\n","        logits_chronic = self.last_linear_chronic(conc)\n","        logits_acute_and_chronic = self.last_linear_acute_and_chronic(conc)\n","        return logits_pe, logits_npe, logits_idt, logits_lpe, logits_rpe, logits_cpe, logits_gte, logits_lt, logits_chronic, logits_acute_and_chronic\n","\n","seq_len = 192\n","feature_size = 2048*3\n","lstm_size = 512\n","image_size = 576\n","batch_size = 12\n","batch_size_lv2 = 8\n","num_partition = 16\n","    \n","model_pe = pe_seresnext101()\n","model_pe.load_state_dict(torch.load('../input/pedetector/seresnext101_splitall/epoch0'))\n","model_pe = model_pe.cuda()\n","model_pe.eval()\n","\n","model_pe1 = pe_seresnext50()\n","model_pe1.load_state_dict(torch.load('../input/pedetector/seresnext50_splitall/epoch0'))\n","model_pe1 = model_pe1.cuda()\n","model_pe1.eval()\n","\n","model_lv2 = lv2Net(input_len=feature_size, lstm_size=lstm_size)\n","model_lv2.load_state_dict(torch.load('../input/lv2detector/splitall/seresnext101_192'))\n","model_lv2 = model_lv2.cuda()\n","model_lv2.eval()\n","\n","model_lv21 = lv2Net(input_len=feature_size, lstm_size=lstm_size)\n","model_lv21.load_state_dict(torch.load('../input/lv2detector/splitall/seresnext50_192'))\n","model_lv21 = model_lv21.cuda()\n","model_lv21.eval()\n","\n","pred_prob_list = []\n","id_list = []\n","series_len_list = []\n","for part in range(num_partition):\n","    if part==num_partition-1:\n","        series_list_part = series_list[part*(len(series_list)//num_partition):]\n","    else:\n","        series_list_part = series_list[part*(len(series_list)//num_partition):(part+1)*(len(series_list)//num_partition)]\n","    \n","    image_list = []\n","    for series_id in series_list_part:\n","        image_list += list(series_dict[series_id]['sorted_image_list'])\n","    print(len(image_list), len(image_dict), len(bbox_dict))\n","\n","    feature = np.zeros((len(image_list), 2048), dtype=np.float32)\n","    feature1 = np.zeros((len(image_list), 2048), dtype=np.float32)\n","\n","    datagen = PEDataset(image_dict=image_dict, bbox_dict=bbox_dict, image_list=image_list, target_size=image_size)\n","    generator = DataLoader(dataset=datagen, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    for i, images in tqdm(enumerate(generator), total=len(generator)):\n","        with torch.no_grad():\n","            start = i*batch_size\n","            end = start+batch_size\n","            if i == len(generator)-1:\n","                end = len(generator.dataset)\n","            images = images.cuda()\n","            features, logits = model_pe(images)\n","            features1, logits1 = model_pe1(images)\n","            feature[start:end] = np.squeeze(features.cpu().data.numpy())\n","            feature1[start:end] = np.squeeze(features1.cpu().data.numpy())\n","    print(feature.shape)\n","\n","    image_to_feature = {}\n","    for i in range(len(feature)):\n","        image_to_feature[image_list[i]] = i\n","    \n","    datagen = lv2Dataset(feature_array=feature,\n","                         feature_array1=feature1,\n","                         image_to_feature=image_to_feature,\n","                         series_dict=series_dict,\n","                         image_dict=image_dict,\n","                         series_list=series_list_part,\n","                         seq_len=seq_len)\n","    generator = DataLoader(dataset=datagen,\n","                           batch_size=batch_size_lv2,\n","                           shuffle=False,\n","                           num_workers=4,\n","                           pin_memory=True)\n","\n","    for j, (x, x1, mask, batch_series_list) in enumerate(generator):\n","        with torch.no_grad():\n","            start = j*batch_size_lv2\n","            end = start+batch_size_lv2\n","            if j == len(generator)-1:\n","                end = len(generator.dataset)\n","            x = x.cuda()\n","            x1 = x1.cuda()\n","            mask = mask.cuda()\n","            logits_pe, logits_npe, logits_idt, logits_lpe, logits_rpe, logits_cpe, logits_gte, logits_lt, logits_chronic, logits_acute_and_chronic = model_lv2(x, mask)\n","            logits_pe1, logits_npe1, logits_idt1, logits_lpe1, logits_rpe1, logits_cpe1, logits_gte1, logits_lt1, logits_chronic1, logits_acute_and_chronic1 = model_lv21(x1, mask)\n","            pred_prob_pe = 0.6*np.squeeze(logits_pe.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_pe1.sigmoid().cpu().data.numpy())\n","            pred_prob_npe = 0.6*np.squeeze(logits_npe.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_npe1.sigmoid().cpu().data.numpy())\n","            pred_prob_idt = 0.6*np.squeeze(logits_idt.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_idt1.sigmoid().cpu().data.numpy())\n","            pred_prob_lpe = 0.6*np.squeeze(logits_lpe.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_lpe1.sigmoid().cpu().data.numpy())\n","            pred_prob_rpe = 0.6*np.squeeze(logits_rpe.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_rpe1.sigmoid().cpu().data.numpy())\n","            pred_prob_cpe = 0.6*np.squeeze(logits_cpe.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_cpe1.sigmoid().cpu().data.numpy())\n","            pred_prob_chronic = 0.6*np.squeeze(logits_chronic.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_chronic1.sigmoid().cpu().data.numpy())\n","            pred_prob_acute_and_chronic = 0.6*np.squeeze(logits_acute_and_chronic.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_acute_and_chronic1.sigmoid().cpu().data.numpy())\n","            pred_prob_gte = 0.6*np.squeeze(logits_gte.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_gte1.sigmoid().cpu().data.numpy())\n","            pred_prob_lt = 0.6*np.squeeze(logits_lt.sigmoid().cpu().data.numpy()) + 0.4*np.squeeze(logits_lt1.sigmoid().cpu().data.numpy())\n","            for n in range(len(batch_series_list)):\n","                pred_prob_list.append(pred_prob_npe[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_negative_exam_for_pe')\n","                pred_prob_list.append(pred_prob_idt[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_indeterminate')\n","                pred_prob_list.append(pred_prob_chronic[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_chronic_pe')\n","                pred_prob_list.append(pred_prob_acute_and_chronic[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_acute_and_chronic_pe')\n","                pred_prob_list.append(pred_prob_cpe[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_central_pe')\n","                pred_prob_list.append(pred_prob_lpe[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_leftsided_pe')\n","                pred_prob_list.append(pred_prob_rpe[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_rightsided_pe')\n","                pred_prob_list.append(pred_prob_gte[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_rv_lv_ratio_gte_1')\n","                pred_prob_list.append(pred_prob_lt[n])\n","                id_list.append(batch_series_list[n].split('_')[0]+'_rv_lv_ratio_lt_1')\n","                num_image = len(series_dict[batch_series_list[n]]['sorted_image_list'])\n","                if num_image>seq_len:\n","                    pred_prob_list += list(np.squeeze(cv2.resize(pred_prob_pe[n, :], (1, num_image), interpolation = cv2.INTER_LINEAR)))\n","                else:\n","                    pred_prob_list += list(pred_prob_pe[n, :num_image])\n","                id_list += list(series_dict[batch_series_list[n]]['sorted_image_list'])\n","                series_len_list.append(len(id_list))\n","                    \n","print(len(id_list), len(pred_prob_list), len(series_len_list))\n","print(id_list[:5])\n","print(pred_prob_list[:5])\n","print(series_len_list[:5])\n","\n","pred_prob_list = np.array(pred_prob_list)\n","series_len_list = np.array(series_len_list)\n","pred_prob_list = correct_predictions(pred_prob_list, series_len_list)\n","print(len(id_list), len(pred_prob_list), len(series_len_list))\n","\n","\n","############################################\n","\n","sub_df = pd.DataFrame(data={'id': id_list, 'label': pred_prob_list})\n","errors = check_consistency(sub_df, df)\n","if len(errors)==0:\n","    sub_df.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}