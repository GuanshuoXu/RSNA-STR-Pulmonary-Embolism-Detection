1542133 1790594 6279
1542133 1790594 6279
1542133 1790594 6279
1542133 1790594 6279
248461 1790594 1000 6279
248461 1790594 1000 6279
248461 1790594 1000 6279
248461 1790594 1000 6279
reduced data:  0.02 30793 pos ratio:  0.05027116552463222
reduced data:  0.02 30793 pos ratio:  0.05027116552463222
reduced data:  0.02 30793 pos ratio:  0.05027116552463222
reduced data:  0.02 30793 pos ratio:  0.05027116552463222
num train steps: 3367
opt
num train steps: 3367
opt
num train steps: 3367
num train steps: 3367
opt
opt
cre
iterator for training
iterator for validation
cre
cre
cre
iterator for training
iterator for validation
iterator for training
iterator for validation
iterator for training
iterator for validation
start trainnnnn
start trainnnnn
start trainnnnn
start trainnnnn
epoch: 0, train_loss: 0.13256713176513404
epoch: 0, train_loss: 0.13903397440275628
epoch: 0, train_loss: 0.14296547375626187
epoch: 0, train_loss: 0.13998941634204043
cnt10 10000 pe 0.0702
loss:0.37086002790157735, auc:0.8175731968214222
cnt10 10000 pe 0.0702
loss:0.37086002790157735, auc:0.8175731968214222
cnt10 10000 pe 0.0702
loss:0.37086002790157735, auc:0.8175731968214222
cnt10 10000 pe 0.0702
loss:0.37086002790157735, auc:0.8175731968214222
checkpoint 0 ...
loss:0.5061997603666523, auc:0.7623077575206744

checkpoint 0 ...
loss:0.5061997603666523, auc:0.7623077575206744

checkpoint 0 ...
loss:0.5061997603666523, auc:0.7623077575206744

best  0.7623077575206744  saving...
checkpoint 0 ...
loss:0.5061997603666523, auc:0.7623077575206744

epoch: 1, train_loss: 0.10395059438311274
epoch: 1, train_loss: 0.11128629870559856
epoch: 1, train_loss: 0.1114424863855128
epoch: 1, train_loss: 0.10735920890407272
cnt10 10000 pe 0.0702
loss:0.24222262584097826, auc:0.8120418017170006
cnt10 10000 pe 0.0702
loss:0.24222262584097826, auc:0.8120418017170006
cnt10 10000 pe 0.0702
loss:0.24222262584097826, auc:0.8120418017170006
cnt10 10000 pe 0.0702
loss:0.24222262584097826, auc:0.8120418017170006
checkpoint 1 ...
loss:0.3268800923269727, auc:0.7413088657039046

checkpoint 1 ...
loss:0.3268800923269727, auc:0.7413088657039046

checkpoint 1 ...
loss:0.3268800923269727, auc:0.7413088657039046

checkpoint 1 ...
loss:0.3268800923269727, auc:0.7413088657039046

epoch: 2, train_loss: 0.09099652320821555
epoch: 2, train_loss: 0.08986838583529391
epoch: 2, train_loss: 0.09295525006511518
epoch: 2, train_loss: 0.0893518837738082
cnt10 10000 pe 0.0702
loss:0.2575695732702887, auc:0.8474548642326659
cnt10 10000 pe 0.0702
loss:0.2575695732702887, auc:0.8474548642326659
cnt10 10000 pe 0.0702
loss:0.2575695732702887, auc:0.8474548642326659
cnt10 10000 pe 0.0702
loss:0.2575695732702887, auc:0.8474548642326659
checkpoint 2 ...
loss:0.3556531045723646, auc:0.8079637758060283

checkpoint 2 ...
loss:0.3556531045723646, auc:0.8079637758060283

checkpoint 2 ...
loss:0.3556531045723646, auc:0.8079637758060283

best  0.8079637758060283  saving...
checkpoint 2 ...
loss:0.3556531045723646, auc:0.8079637758060283

epoch: 3, train_loss: 0.06892474828631988
epoch: 3, train_loss: 0.07351309262121253
epoch: 3, train_loss: 0.07250370611686462
epoch: 3, train_loss: 0.06814796095301477
cnt10 10000 pe 0.0702
loss:0.45782206699617345, auc:0.7828209080897831
cnt10 10000 pe 0.0702
loss:0.45782206699617345, auc:0.7828209080897831
cnt10 10000 pe 0.0702
loss:0.45782206699617345, auc:0.7828209080897831
cnt10 10000 pe 0.0702
loss:0.45782206699617345, auc:0.7828209080897831
checkpoint 3 ...
loss:0.6674023287204808, auc:0.6899025680913062

checkpoint 3 ...
loss:0.6674023287204808, auc:0.6899025680913062

checkpoint 3 ...
loss:0.6674023287204808, auc:0.6899025680913062

checkpoint 3 ...
loss:0.6674023287204808, auc:0.6899025680913062

epoch: 4, train_loss: 0.05339048359735188
epoch: 4, train_loss: 0.056367122100479936
epoch: 4, train_loss: 0.05808605686952264
epoch: 4, train_loss: 0.054067219971823226
cnt10 10000 pe 0.0702
loss:0.5226008174055626, auc:0.8131131652856753
cnt10 10000 pe 0.0702
loss:0.5226008174055626, auc:0.8131131652856753
cnt10 10000 pe 0.0702
loss:0.5226008174055626, auc:0.8131131652856753
cnt10 10000 pe 0.0702
loss:0.5226008174055626, auc:0.8131131652856753
checkpoint 4 ...
loss:0.7494003392838097, auc:0.7317812375678214

checkpoint 4 ...
loss:0.7494003392838097, auc:0.7317812375678214

checkpoint 4 ...
loss:0.7494003392838097, auc:0.7317812375678214

checkpoint 4 ...
loss:0.7494003392838097, auc:0.7317812375678214

epoch: 5, train_loss: 0.039629867059259215
epoch: 5, train_loss: 0.04026030822243221
epoch: 5, train_loss: 0.03829313984377485
epoch: 5, train_loss: 0.03935738119759669
cnt10 10000 pe 0.0702
loss:0.4939805912309861, auc:0.8086949587541111
cnt10 10000 pe 0.0702
loss:0.4939805912309861, auc:0.8086949587541111
cnt10 10000 pe 0.0702
loss:0.4939805912309861, auc:0.8086949587541111
cnt10 10000 pe 0.0702
loss:0.4939805912309861, auc:0.8086949587541111
checkpoint 5 ...
loss:0.7537556585235473, auc:0.7079865769504236

checkpoint 5 ...
loss:0.7537556585235473, auc:0.7079865769504236

checkpoint 5 ...
loss:0.7537556585235473, auc:0.7079865769504236

checkpoint 5 ...
loss:0.7537556585235473, auc:0.7079865769504236

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
